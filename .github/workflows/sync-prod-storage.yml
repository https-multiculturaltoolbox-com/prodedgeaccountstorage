name: Sync Azure Blob (chunked) to Repo

on:
  workflow_dispatch:
    inputs:
      prefixes:
        description: "Space-separated prefixes (use '.' for whole container)"
        required: true
        default: "."

permissions:
  contents: write

env:
  AZURE_STORAGE_ACCOUNT: ${{ secrets.AZURE_STORAGE_ACCOUNT }}
  AZURE_CONTAINER: ${{ secrets.AZURE_CONTAINER }}

jobs:
  sync:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Configure git author
        run: |
          git config user.name  "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"

      # Optional but strongly recommended for large syncs on GitHub-hosted runners
      - name: Cleanup disk space before sync
        run: |
          sudo rm -rf /usr/share/dotnet /opt/ghc "/usr/local/share/boost" || true
          sudo apt-get clean || true
          df -h

      - name: Install AzCopy (v10)
        run: |
          set -e
          curl -sL https://aka.ms/downloadazcopy-v10-linux | tar -xz --strip-components=1
          sudo mv azcopy /usr/local/bin/azcopy
          azcopy --version

      - name: Install Azure CLI & jq (for discovery/paging)
        run: |
          set -e
          curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash
          sudo apt-get update
          sudo apt-get install -y jq
          az --version

      - name: Sync (chunked, root-safe, low-disk)
        env:
          PREFIXES: ${{ github.event.inputs.prefixes }}
          AZURE_STORAGE_KEY: ${{ secrets.AZURE_STORAGE_KEY }}
        run: |
          set -euo pipefail
          : "${AZURE_STORAGE_KEY:?AZURE_STORAGE_KEY missing}"
          : "${AZURE_STORAGE_ACCOUNT:?AZURE_STORAGE_ACCOUNT missing}"
          : "${AZURE_CONTAINER:?AZURE_CONTAINER missing}"
          : "${PREFIXES:?No prefixes provided}"

          ulimit -n 65536 || true

          # AzCopy auth & tuning (env var replaces removed CLI flag)
          export AZCOPY_ACCOUNT_KEY="${AZURE_STORAGE_KEY}"
          export AZCOPY_CONCURRENCY_VALUE=8
          export AZCOPY_LOG_LOCATION="${RUNNER_TEMP}/azcopy-logs"
          mkdir -p "$AZCOPY_LOG_LOCATION"

          # Start with a minimal worktree to keep disk usage low
          git sparse-checkout init --cone
          git sparse-checkout set .github

          tmpdir="$(mktemp -d)"
          trap 'rm -rf "$tmpdir"' EXIT

          push_chunk () {
            local path_in_repo="$1"

            # Skip >95MB blobs to avoid GitHub's 100MB hard limit
            local BIG
            BIG=$(find "$path_in_repo" -type f -size +95M 2>/dev/null || true)
            git add "$path_in_repo"
            if [ -n "${BIG:-}" ]; then
              echo "::warning::Skipping >95MB files under $path_in_repo" >&2
              echo "$BIG" | sed 's/^/ - /' >&2
              echo "$BIG" | xargs -r git reset -q
            fi

            if ! git diff --cached --quiet -- "$path_in_repo"; then
              git commit -m "Sync from Azure ${path_in_repo} - $(date -u +'%Y-%m-%d %H:%M:%S UTC')"
              git push
            else
              echo "No changes in $path_in_repo" >&2
            fi

            df -h
          }

          # Emit ONLY real prefixes to STDOUT; all chatter -> STDERR
          # Also capture root files into $tmpdir/root-files.txt (one per line)
          generate_full_prefix_list () {
            echo "Discovering top-level prefixes and root files (paged)..." >&2
            : > "$tmpdir/prefixes.txt"
            : > "$tmpdir/root-files.txt"

            local marker=""
            while :; do
              # Note: Azure CLI prints Next Marker to STDERR; capture it
              local out="$tmpdir/page.json" err="$tmpdir/page.err"
              if [ -z "$marker" ]; then
                az storage blob list \
                  --account-name "$AZURE_STORAGE_ACCOUNT" \
                  --account-key  "$AZURE_STORAGE_KEY" \
                  --container-name "$AZURE_CONTAINER" \
                  --delimiter "/" --num-results 5000 \
                  -o json >"$out" 2>"$err"
              else
                az storage blob list \
                  --account-name "$AZURE_STORAGE_ACCOUNT" \
                  --account-key  "$AZURE_STORAGE_KEY" \
                  --container-name "$AZURE_CONTAINER" \
                  --delimiter "/" --num-results 5000 \
                  --marker "$marker" \
                  -o json >"$out" 2>"$err"
              fi

              # Top-level prefixes (virtual folders ending with '/')
              jq -r '.. | .blobPrefixes? // empty | .[]?.name' "$out" >> "$tmpdir/prefixes.txt" || true
              # Root files (those without '/'); with delimiter, CLI returns them in the top-level array
              jq -r '.[]? | select(.name) | .name' "$out" >> "$tmpdir/root-files.txt" || true

              # Next Marker (CLI writes: "WARNING: Next Marker: <token>")
              marker="$(sed -n 's/^WARNING: Next Marker:[[:space:]]*//p' "$err" | tail -n1 || true)"
              if [ -z "$marker" ]; then break; fi
            done

            # Emit prefixes (one per line) to STDOUT
            if [ -s "$tmpdir/prefixes.txt" ]; then
              cat "$tmpdir/prefixes.txt"
            fi
            # If there are root files, emit the special marker
            if [ -s "$tmpdir/root-files.txt" ]; then
              echo "__ROOT__"
            fi
          }

          HAD_ERRORS=0

          if [ "$PREFIXES" = "." ]; then
            CHUNKS="$(generate_full_prefix_list)"
            if [ -z "$CHUNKS" ]; then
              echo "::notice::No prefixes or root files discovered; nothing to sync."
              exit 0
            fi
          else
            CHUNKS="$PREFIXES"
          fi

          # Process each discovered chunk
          for p in $CHUNKS; do
            if [ "$p" = "__ROOT__" ]; then
              echo "==> Processing ROOT files in batches" >&2
              SRC_URL="https://${AZURE_STORAGE_ACCOUNT}.blob.core.windows.net/${AZURE_CONTAINER}"

              BATCH_SIZE=25   # small include list avoids HTTP 414
              mapfile -t ROOTFILES < "$tmpdir/root-files.txt" || true
              total=${#ROOTFILES[@]}
              if [ "$total" -eq 0 ]; then
                echo "No root files." >&2
              else
                i=0
                while [ $i -lt $total ]; do
                  batch=( "${ROOTFILES[@]:$i:$BATCH_SIZE}" )
                  i=$(( i + BATCH_SIZE ))

                  # CSV for --include-path (names are assumed to be simple; if you have spaces, consider escaping/quoting here)
                  inc="$(printf "%s," "${batch[@]}")"
                  inc="${inc%,}"

                  dest="$tmpdir/__root_batch__$i"
                  mkdir -p "$dest"

                  echo "   - Downloading ROOT batch ending at index $i (size ${#batch[@]})" >&2
                  set +e
                  azcopy copy "$SRC_URL" "$dest" --recursive=false --include-path "$inc" --log-level INFO
                  AC_STATUS=$?
                  set -e

                  # Switch to non-cone sparse mode to include only this batch (plus .github)
                  git sparse-checkout init --no-cone
                  # Build patterns: keep workflows + current batch files at repo root
                  patterns=( "/.github/*" )
                  for f in "${batch[@]}"; do patterns+=( "/$f" ); done
                  git sparse-checkout set "${patterns[@]}"

                  rsync -a "$dest"/ "./" || true
                  push_chunk "."

                  # Evict back to minimal worktree
                  git sparse-checkout init --cone
                  git sparse-checkout set .github
                  rm -rf "$dest"

                  if [ "$AC_STATUS" -ne 0 ]; then
                    HAD_ERRORS=1
                    echo "::warning::AzCopy had errors in a ROOT batch; continuing." >&2
                    tail -n 100 "$(ls -t "$AZCOPY_LOG_LOCATION"/*.log | head -n1)" || true
                  fi
                done
              fi
              continue
            fi

            # Normal prefix (folder)
            echo "==> Processing prefix: $p" >&2
            SRC_URL="https://${AZURE_STORAGE_ACCOUNT}.blob.core.windows.net/${AZURE_CONTAINER}/${p}"
            DEST_LOCAL="${tmpdir}/${p}"
            TARGET_DIR="./$p"

            mkdir -p "$DEST_LOCAL" "$TARGET_DIR"

            set +e
            azcopy copy "$SRC_URL" "$DEST_LOCAL" --recursive --log-level INFO
            AC_STATUS=$?
            set -e

            # Include this prefix in sparse checkout so git will stage it
            git sparse-checkout set .github "$p"

            rsync -a "$DEST_LOCAL"/ "$TARGET_DIR"/ || true
            push_chunk "$TARGET_DIR"

            # Evict this chunk to save disk
            git sparse-checkout set .github || true
            rm -rf "$DEST_LOCAL"

            if [ "$AC_STATUS" -ne 0 ]; then
              HAD_ERRORS=1
              echo "::warning::AzCopy returned exit $AC_STATUS for $p; see logs in $AZCOPY_LOG_LOCATION" >&2
              tail -n 100 "$(ls -t "$AZCOPY_LOG_LOCATION"/*.log | head -n1)" || true
            fi
          done

          echo "All requested chunks processed."
          # Fail the job if any AzCopy errors occurred (optional)
          # [ "$HAD_ERRORS" -ne 0 ] && exit 1 || true

      - name: Final disk report
        if: always()
        run: df -h
